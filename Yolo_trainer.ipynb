{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "model = YOLO(\"models/yolov8l.pt\")\n",
    "\n",
    "# Print the current working directory\n",
    "current_path = os.getcwd()\n",
    "print(\"Current working directory:\", current_path)\n",
    "\n",
    "# Define the dataset path relative to the current working directory\n",
    "dataset_path = os.path.join(current_path, 'src', 'data', 'data.yaml')\n",
    "#results = model.train(data=dataset_path, epochs=50, imgsz=640, batch=32)\n",
    "results = model.train(data=dataset_path, epochs=5, imgsz=640, batch=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this to check whether you have GPU (cuda) or CPU active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Training will use the GPU.\")\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Training will use the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to run inference on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "#model = YOLO(\"models/best_v10.pt\")\n",
    "model = YOLO(\"models/best_v8.pt\")\n",
    "\n",
    "# Print the current working directory\n",
    "current_path = os.getcwd()\n",
    "print(\"Current working directory:\", current_path)\n",
    "\n",
    "# Define the test images folder path relative to the current working directory\n",
    "test_images_folder = os.path.join(current_path, 'src', 'data', 'test_set', 'cropped')\n",
    "\n",
    "image_files = [os.path.join(test_images_folder, \"testikuva2.jpg\")]\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = model(image_files)  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "\n",
    "\n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to run inference on a whole folder of test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def run_inference_on_folder(model_path, source_folder, dest_folder):\n",
    "    # Load the model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Ensure destination folder exists\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    # Get the list of image files in the source folder\n",
    "    image_files = [os.path.join(source_folder, img) for img in os.listdir(source_folder) if img.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # Run batched inference on a list of images\n",
    "    results = model(image_files)  # return a list of Results objects\n",
    "\n",
    "    # Process results list and save images with inference results\n",
    "    for img_path, result in zip(image_files, results):\n",
    "        img_name = os.path.basename(img_path)\n",
    "        result_img_path = os.path.join(dest_folder, img_name)\n",
    "\n",
    "        # Save the image with inference results\n",
    "        result.save(result_img_path)\n",
    "        print(f\"Saved inference result for {img_name} to {result_img_path}\")\n",
    "\n",
    "def main():\n",
    "    # Model path\n",
    "    model_path = 'models/best_v8l.pt'\n",
    "\n",
    "    # Source folder containing cropped images\n",
    "    source_folder = os.path.join(os.getcwd(), 'src', 'data', 'test_set', 'cropped')\n",
    "\n",
    "    # Destination folder to save images with inference results\n",
    "    dest_folder = os.path.join(os.getcwd(), 'src', 'data', 'test_set', 'inference_results_v8l')\n",
    "\n",
    "    # Run inference on the folder of cropped images\n",
    "    run_inference_on_folder(model_path, source_folder, dest_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.69  Python-3.9.19 torch-2.3.1 CUDA:0 (NVIDIA GeForce RTX 3080, 10239MiB)\n",
      "Model summary (fused): 268 layers, 43,641,303 parameters, 0 gradients, 165.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\35850\\Documents\\1_lego_detection\\LEGO_sorter_rasmus\\src\\data\\testikuvat\\labels.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40        143      0.972      0.975       0.98      0.787\n",
      "           0001_grey_a          5          5      0.986          1      0.995      0.796\n",
      "           0001_grey_b          1          1          1          1      0.995      0.697\n",
      "           0001_grey_c          2          2          1          1      0.995      0.635\n",
      "          0002_black_a          3          3          1          1      0.995      0.763\n",
      "          0002_black_u          3          3      0.741          1      0.995      0.666\n",
      "         0002_yellow_a          2          2      0.963          1      0.995       0.73\n",
      "         0002_yellow_u          3          3          1          1      0.995      0.686\n",
      "          0003_black_a          1          1       0.95          1      0.995      0.697\n",
      "          0003_black_u          1          1          1          0      0.332      0.199\n",
      "         0003_orange_a          6          6          1          1      0.995      0.785\n",
      "         0003_orange_u          2          2      0.968          1      0.995      0.627\n",
      "         0004_yellow_a          3          3          1          1      0.995      0.774\n",
      "         0004_yellow_b          5          5      0.985          1      0.995      0.783\n",
      "         0004_yellow_c          2          2          1          1      0.995      0.771\n",
      "          0005_black_a          4          4          1          1      0.995      0.787\n",
      "          0005_black_b          2          2          1          1      0.995      0.759\n",
      "          0005_black_c          3          3          1          1      0.995      0.719\n",
      "         0006_yellow_a          6          6      0.991          1      0.995      0.827\n",
      "         0006_yellow_u          4          4          1          1      0.995        0.8\n",
      "         0007_yellow_a          7          7          1          1      0.995      0.875\n",
      "         0007_yellow_b          5          5          1          1      0.995      0.784\n",
      "         0008_yellow_a          5          5          1          1      0.995      0.786\n",
      "         0008_yellow_b          3          3          1          1      0.995      0.797\n",
      "         0009_yellow_a          3          3      0.974          1      0.995      0.866\n",
      "         0009_yellow_u          3          3          1          1      0.995      0.895\n",
      "         0010_yellow_a          2          2      0.969          1      0.995      0.895\n",
      "         0010_yellow_b          1          1          1          1      0.995      0.895\n",
      "         0010_yellow_c          1          1          1          1      0.995      0.895\n",
      "         0011_yellow_a          5          5      0.984          1      0.995      0.939\n",
      "         0011_yellow_b          2          2      0.967          1      0.995      0.895\n",
      "         0011_yellow_c          1          1          1          1      0.995      0.796\n",
      "          0012_beige_a          7          7          1      0.869      0.995       0.85\n",
      "           0013_grey_a          7          7      0.972          1      0.995      0.932\n",
      "           0014_grey_a          2          2          1          1      0.995      0.796\n",
      "           0014_grey_b          2          2          1          1      0.995      0.796\n",
      "           0014_grey_c          1          1      0.951          1      0.995      0.697\n",
      "           0014_grey_d          4          4       0.98          1      0.995      0.811\n",
      "         0015_yellow_a          4          4      0.787          1      0.995      0.908\n",
      "         0015_yellow_b          2          2      0.667          1      0.995      0.854\n",
      "       0016_darkgrey_a          4          4      0.979          1      0.995      0.895\n",
      "       0016_darkgrey_b          2          2      0.966          1      0.995      0.829\n",
      "           0017_grey_a          5          5          1          1      0.995      0.826\n",
      "           0017_grey_b          3          3      0.977          1      0.995      0.725\n",
      "           0018_grey_a          2          2          1          1      0.995      0.858\n",
      "           0018_grey_b          2          2          1          1      0.995      0.834\n",
      "Speed: 2.7ms preprocess, 18.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n",
      "0.7873763620009618\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"models/best_v8l.pt\")\n",
    "\n",
    "# Validate model\n",
    "# Validate the model\n",
    "metrics = model.val(data='C:/Users/35850/Documents/1_lego_detection/LEGO_sorter_rasmus/src/data/testikuvat/test_data.yaml', iou=0.5)\n",
    "print(metrics.box.map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
